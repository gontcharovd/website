---
title: "Updating Spark Configuration Values"
date: '2025-08-24'
issueno: "024"
draft: false
---

After a brief summer break, I started exploring the Spark UI. The first steps I took are exploring changing Spark configuration parameters such as `spark.sql.shuffle.partitions`.

I can update the default value by adding a `config()` statement in the Scala code:

```Scala
val spark = SparkSession
    .builder()
    .appName("optimization")
    .config("spark.sql.shuffle.partitions", "300")  // <-- Here
    .master("local[*]")
    .getOrCreate()
```

Querying the value of this parameters, shows that it's indeed 300.
